            +--------------------+
            | CSE311             |
            | PROJECT 1: THREADS |
            | DESIGN DOCUMENT    |
            +--------------------+
   
---- GROUP ----

Dusan Baek <santoo@unist.ac.kr>
Woocheol Shin <woofe@unist.ac.kr>


---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other thionan the Pintos documentat, course
>> text, lecture notes, and course staff.

Resources Provided:

All the lecture materials (PPT, PDF, YouTube lectures)
Alarm clock & priority scheduling: https://www.youtube.com/watch?v=myO2bs5LMak
MLFQ: https://www.youtube.com/watch?v=4-OjMqyygss
Context switching: https://www.youtube.com/watch?v=mtX-bj1Fu6M

Online References:

https://one-step-a-day.tistory.com/126
https://poalim.tistory.com/28


                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread {
  ...
  int64_t tick_wakeup;
  ...
};
: Holds the time each thread needs to wake up.

static struct list sleep_list;
: Sleep list for efficient CPU resource use. Threads yield to this list.

int64_t global_wakeup_tick = INT64_MAX;
: Global variable to store the earliest wakeup time to avoid excessive computations by the timer interrupt handler.
---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

The function calculates the wakeup time based on the current tick count and the ticks specified in the parameters.
If the elapsed time is less than the sleep duration, thread_sleep() is invoked to make the thread sleep for a specific time.
thread_sleep() disables interrupts for atomicity, changes the thread's state to "blocked," and stores its wakeup time in tick_wakeup.
The thread is then added to the sleep_list, followed by a context switch.
The timer interrupt handler increases the global tick count to keep track of system time.
The handler also checks the global_wakeup_tick and executes thread_wakeup() to send sleeping threads from the sleep_list to the ready_list.
thread_wakeup() traverses the sleep_list, waking threads whose wakeup time matches or exceeds the global tick count.
The threads are moved to the ready_list using thread_unblock().

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

The 'global_wakeup_tick' variable quickly identifies whether any threads should be woken up.
Keeping the 'global_wakeup_tick' updated when threads sleep is an O(1) complexity operation,
whereas not doing so could lead to O(n) complexity. This ensures optimal performance.
The 'sleep_list' is sorted, so waking threads iterates sequentially until finding the first threads
whos wakeup time exceeds the global tick count.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Use 'intr_disable()' when entering critical sections during 'timer_sleep()',
and enable interrupts by 'intr_set_level()'.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Interrupts are disabled during 'timer_sleep()',
preventing the timer interrupt handler from running simultaneously.


---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Reducing busy waiting allows better CPU utilization and energy efficiency.
The sorted 'sleep_list' and 'global_wakeup_tick' minimize computational overhead.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread {
  ...
  int original_priority;
  struct lock *wait_lock;
  struct list_elem d_elem;
  struct list donations;
  ...
}
original_priority: Stores the thread's original priority before donation.
wait_lock: The lock the thread is currently waiting for.
d_elem: List element for donation purposes.
donations: List for managing multiple priority donations.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

Scheduling involves CPU and resource scheduling.

In CPU scheduling,
When a thread yields or is blocked, it dequeues the highest-priority thread from the ready queue.
The ready queue is sorted in descending priority order.
Sorting happens during yielding, blocking, priority change ('thread_set_priority()'), and thread creation ('thread_create()').

Likewise, in resource scheduling,
Locks and semaphores use 'list_insert_ordered()' on 'down()' and sort the whole list on 'up()'.
Condition variables use 'list_insert_ordered()' during 'cond_wait()' and sort during 'cond_signal()'.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When a thread calls lock_acquire() and the lock is held by another thread:
The current thread records the lock it is waiting for (thread_current()->wait_lock).
It adds its priority to the holding thread's donation list and sorts it in descending order.
The 'priority_donate()' function donates the priority to the lock-holding thread if the latter's priority is lower.
If the holding thread itself is waiting for another lock, the donation is recursively propagated via 'priority_donate()' to resolve priority inversion.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

lock_release() causes low-priority threads to return to the state they were in before taking over priority,
in the opposite way to how 'lock_acquire()' occurs.
'lock_release()' ensures that, when the lock is released, the thread returns to its original priority
using the value stored in original_priority. The donations list is also cleared.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

The potential race condition occurs when multiple threads attempt to set threads' priorities simultaneously.
Our codes sorts the priority of ready_list between 'intr_disable()' and 'intr_set_level()' atomically, so it
can prevent race condition. But you can also use other functions like semaphore or lock.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Executing more important tasks first ensures excellent performance.
Maintaining descending order in various situations where priority changes increases CPU performance.
Additionally, even at the moment a thread needs a resource, if another thread is occupying that resource,
it may remain in a waiting state even though it is a high priority thread.
If you lend priority to a thread that occupies the necessary resources to finish the task quickly and release the lock early,
you can shorten the completion time of the entire task, satisfy real-time processing demands,
and reduce the resource occupancy time of a specific thread. By shortening it, deadlock can be prevented.

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread {
  int nice;
  int recent_cpu;
}
nice: The thread's niceness value, ranging from -20 to 19.
recent_cpu: Measures recent CPU usage by this thread.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59   A
 4      4   0   0  62  61  59   A
 8      8   0   0  61  61  59   B
12      8   4   0  61  60  59   A
16     12   4   0  60  60  59   B
20     12   8   0  60  59  59   A
24     16   8   0  59  59  59   C
28     16   8   4  59  59  58   B
32     16  12   4  59  58  58   A
36     20  12   4  58  58  58   C


>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Every second, load_avg must be determined and the recent_cpu value of all threads must be recalculated.
However, in the case of the above graph, a value of up to 36 ticks is required, and since it corresponds to about 100 ticks per second,
the change in the value of recent_cpu due to load_avg is not taken into consideration.

There is a way to continue executing the currently executing thread when it has the same priority,
and a way to hand it over to a waiting thread. In the graph above, when the priority is the same,
it is written in a way to hand it over to another thread.
This is because in mlfqs, processing between threads with the same priority is generally done in a round-robin manner.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

Calculating recent_cpu and load_avg also uses CPU resources,
so if many calculations are performed in a situation where interrupts are disabled (when the CPU is occupied),
it may lead to a decrease in performance.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

Floating point operations degrade kernel performance because they involve complex operations.
For pintos, which do not support floating point arithmetic, you must implement floating point arithmetic yourself.
We implemented floating-point operations through bitwise operations with the first bit as the sign, 17 bits as the integer part, and 14 bits as the decimal part in 32 bits.

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?

